
# Domain: Banking Big Data Platform

**Project Overview**  
This project demonstrates a scalable, enterprise-grade banking data platform using modern Data Engineering and DevOps best practices. It ingests, processes, models, and serves banking data at scale using **AWS, Snowflake, Spark, Airflow, dbt, Data Vault, Docker, Terraform, and Kubernetes (K8s).**

---

## 1. Architecture Overview

<img width="1536" height="1024" alt="ChatGPT Image Jan 3, 2026, 05_10_06 PM" src="https://github.com/user-attachments/assets/ba3d45c7-b0da-4ffe-97d5-668de5d53d4a" />



**Components & Flow:**

| Layer | Tool / Technology | Description |
|-------|-----------------|-------------|
| Data Ingestion | Spark / AWS S3 / Kafka | Batch & real-time ingestion from core banking systems, payment gateways, and CRM. |
| Data Lake / Bronze Layer | AWS S3 | Raw storage layer (immutable, append-only) |
| Data Processing / Silver | Spark / Databricks | Cleansing, validation, transformations, enrichment |
| Data Vault Layer | dbt + Snowflake | Implemented following **Data Vault 2.0** methodology for historization and auditability |
| Orchestration | Airflow | Manage ETL workflows, scheduling, monitoring |
| Infrastructure | Terraform | Automated provisioning of AWS, Snowflake, S3, and K8s clusters |
| Deployment | Docker + Kubernetes | Containerized workflows for Spark jobs, Airflow workers, dbt runs |
| Analytics & Reporting | Snowflake + BI tools | Secure, governed access for analysts & business teams |

---

## 2. Project Folder Structure

```mermaid

graph TD
    A[Push/PR] --> B[CI Pipeline]
    B --> C[Code Quality]
    B --> D[Python Tests]
    B --> E[Airflow Validation]
    B --> F[dbt Validation]
    B --> G[SQL Lint]
    B --> H[Security Scan]
    B --> I[Docker Build]
    
    C --> J[CI Success]
    D --> J
    E --> J
    F --> J
    G --> J
    H --> J
    I --> J
    
    K[Terraform Changes] --> L[Terraform Validate]
    L --> M[Security Scan]
    M --> N[Plan DEV]
    M --> O[Plan PROD]
    
    N --> P[Apply DEV - Auto]
    O --> Q[Apply PROD - Manual Approval]
    
    R[Main Branch] --> S[Build Docker Images]
    S --> T[Push to Registry]

```
```
NILOOMID-banking-data-platform/
AWS_Snowflake_DBT__Project/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â””â”€â”€ terraform-plan.yml
â”œâ”€â”€ .gitignore                          # âœ… MUST CREATE
â”œâ”€â”€ README.md                           # âœ… UPDATE
â”œâ”€â”€ requirements.txt                    # âœ… UPDATE
â”œâ”€â”€ .env.example                        # âœ… CREATE (not .env)
â”œâ”€â”€ profiles.yml.example                # âœ… CREATE (not profiles.yml)
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ images/
â”‚   â”‚   â””â”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ SETUP.md                        # âœ… CREATE
â”‚   â”œâ”€â”€ DATA_VAULT_DESIGN.md           # âœ… CREATE
â”‚   â””â”€â”€ API_DOCUMENTATION.md
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â””â”€â”€ terraform/
â”‚       â”œâ”€â”€ modules/                    # âœ… CREATE MODULE STRUCTURE
â”‚       â”‚   â”œâ”€â”€ aws/
â”‚       â”‚   â”‚   â”œâ”€â”€ s3/
â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚       â”‚   â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚       â”‚   â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚       â”‚   â”‚   â””â”€â”€ eks/
â”‚       â”‚   â”‚       â”œâ”€â”€ main.tf
â”‚       â”‚   â”‚       â”œâ”€â”€ variables.tf
â”‚       â”‚   â”‚       â””â”€â”€ outputs.tf
â”‚       â”‚   â””â”€â”€ snowflake/
â”‚       â”‚       â”œâ”€â”€ database/
â”‚       â”‚       â”‚   â”œâ”€â”€ main.tf
â”‚       â”‚       â”‚   â”œâ”€â”€ variables.tf
â”‚       â”‚       â”‚   â””â”€â”€ outputs.tf
â”‚       â”‚       â””â”€â”€ warehouse/
â”‚       â”‚           â”œâ”€â”€ main.tf
â”‚       â”‚           â”œâ”€â”€ variables.tf
â”‚       â”‚           â””â”€â”€ outputs.tf
â”‚       â””â”€â”€ env/                        # âœ… CREATE ENVIRONMENT STRUCTURE
â”‚           â”œâ”€â”€ dev/
â”‚           â”‚   â”œâ”€â”€ main.tf
â”‚           â”‚   â”œâ”€â”€ variables.tf
â”‚           â”‚   â”œâ”€â”€ backend.tf
â”‚           â”‚   â”œâ”€â”€ terraform.tfvars.example
â”‚           â”‚   â””â”€â”€ providers.tf
â”‚           â””â”€â”€ prod/
â”‚               â”œâ”€â”€ main.tf
â”‚               â”œâ”€â”€ variables.tf
â”‚               â”œâ”€â”€ backend.tf
â”‚               â”œâ”€â”€ terraform.tfvars.example
â”‚               â””â”€â”€ providers.tf
â”‚
â”œâ”€â”€ k8s/                                # âœ… MOVE TO ROOT (per your README)
â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â””â”€â”€ configmap.yaml
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â””â”€â”€ spark-job.yaml
â”‚   â””â”€â”€ dbt/
â”‚       â””â”€â”€ dbt-runner.yaml
â”‚
â”œâ”€â”€ dags/                               # Airflow DAGs
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ s3_to_snowflake_ingest.py      # âœ… RENAME from ingestion_dag.py
â”‚   â”œâ”€â”€ dbt_vault_run.py               # âœ… RENAME from dbt_dag.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ snowflake_helpers.py
â”‚
â”œâ”€â”€ dbt/                                # dbt Core Project
â”‚   â”œâ”€â”€ dbt_project.yml                # âœ… FIX CONFIGURATION
â”‚   â”œâ”€â”€ packages.yml                   # âœ… ADD DBT PACKAGES
â”‚   â”œâ”€â”€ selectors.yml                  # âœ… CREATE
â”‚   â”œâ”€â”€ profiles.yml.example           # âœ… CREATE TEMPLATE
â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â”œâ”€â”€ generate_schema_name.sql
â”‚   â”‚   â”œâ”€â”€ hash_key.sql
â”‚   â”‚   â””â”€â”€ data_masking.sql
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”œâ”€â”€ _sources.yml           # âœ… SOURCE DEFINITIONS
â”‚   â”‚   â”‚   â”œâ”€â”€ _staging.yml           # âœ… MODEL DOCUMENTATION
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_customers.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ stg_accounts.sql
â”‚   â”‚   â”‚   â””â”€â”€ stg_transactions.sql
â”‚   â”‚   â”œâ”€â”€ vault/
â”‚   â”‚   â”‚   â”œâ”€â”€ hubs/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ _hubs.yml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hub_customer.sql
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ hub_account.sql
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ hub_transaction.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ links/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ _links.yml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ link_customer_account.sql
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ link_account_transaction.sql
â”‚   â”‚   â”‚   â””â”€â”€ satellites/
â”‚   â”‚   â”‚       â”œâ”€â”€ _satellites.yml
â”‚   â”‚   â”‚       â”œâ”€â”€ sat_customer_details.sql
â”‚   â”‚   â”‚       â”œâ”€â”€ sat_account_details.sql
â”‚   â”‚   â”‚       â””â”€â”€ sat_transaction_details.sql
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚       â”œâ”€â”€ _marts.yml
â”‚   â”‚       â””â”€â”€ mart_customer_360.sql
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ generic/
â”‚   â”‚   â””â”€â”€ singular/
â”‚   â”œâ”€â”€ seeds/
â”‚   â”‚   â””â”€â”€ country_codes.csv
â”‚   â””â”€â”€ snapshots/
â”‚
â”œâ”€â”€ spark_jobs/                         # PySpark ETL Jobs
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bronze_ingestion.py            # Raw data ingestion
â”‚   â”œâ”€â”€ silver_transform.py            # Data cleansing
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ spark_config.py
â”‚   â”‚   â””â”€â”€ data_quality.py
â”‚   â””â”€â”€ tests/
â”‚       â””â”€â”€ test_transformations.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ airflow.Dockerfile             # âœ… RENAME (consistent naming)
â”‚   â”œâ”€â”€ dbt.Dockerfile
â”‚   â”œâ”€â”€ spark.Dockerfile
â”‚   â””â”€â”€ docker-compose.yml             # âœ… CREATE
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ setup_env.sh                   # Environment setup
    â”œâ”€â”€ deploy.sh                      # Deployment script
    â””â”€â”€ init_snowflake.sql             # Snowflake initialization
````

---

## 3. Key Features

- **Data Vault 2.0**: Fully auditable, historized enterprise banking model.
- **Cloud-Native**: AWS S3, Snowflake, K8s-managed Spark and Airflow.
- **Orchestrated Pipelines**: Airflow DAGs for batch and streaming ingestion.
- **Infrastructure as Code (IaC)**: Terraform scripts for reproducible environments.
- **Modular dbt Models**: Hub, Link, Satellite layers for clean data modeling.
- **Containerized Deployment**: Docker + K8s for scalable, portable workloads.
- **Big Data Processing**: Spark handles large transaction and payment datasets efficiently.
- **Security & Governance**: Snowflake role-based access, audit logs, encrypted data at rest.

---

## 4. Getting Started

### Prerequisites

- AWS Account (S3, EC2, EKS)
- Docker & Kubernetes
- Terraform 1.5+
- Python 3.11+
- Snowflake Account
- dbt Core 1.7+
- Apache Airflow 2.8+

### Steps

1. Clone repository:  
```bash
git clone https://github.com/NILOOMID/banking-data-platform.git
cd banking-data-platform
````

2. Provision infrastructure:

```bash
cd infrastructure/terraform
terraform init
terraform apply
```

3. Deploy K8s workloads:

```bash
kubectl apply -f ../k8s/
```

4. Build and run Docker images:

```bash
docker build -t airflow ./docker/Dockerfile-airflow
docker build -t spark ./docker/Dockerfile-spark
docker build -t dbt ./docker/Dockerfile-dbt
```

5. Trigger Airflow DAGs:

```bash
airflow dags list
airflow dags trigger ingestion_dag
```

6. Run dbt transformations:

```bash
dbt run --project-dir dbt
```

---

## 5. Data Vault Structure

| Type      | Example Table         | Description                            |
| --------- | --------------------- | -------------------------------------- |
| Hub       | hub_customer          | Core entity (customer, account, card)  |
| Link      | link_customer_account | Relationships between hubs             |
| Satellite | sat_customer_profile  | Historical attributes and transactions |

---

## 6. CI/CD

* **GitHub Actions**: Runs dbt tests, Airflow DAG lint, Terraform plan.
* **Docker Hub**: Push container images.
* **K8s Rollouts**: Canary deployments for Spark jobs and Airflow workers.

---

## 7. Monitoring & Logging

* Airflow UI & logs
* Spark job metrics via Prometheus & Grafana
* Snowflake Query History
* CloudWatch logs

---
ğŸ” Required Secrets Setup
Run these commands to add all necessary secrets:
bash# AWS Credentials
gh secret set AWS_ACCESS_KEY_ID_DEV
gh secret set AWS_SECRET_ACCESS_KEY_DEV
gh secret set AWS_ACCESS_KEY_ID_PROD
gh secret set AWS_SECRET_ACCESS_KEY_PROD

# Snowflake Credentials
gh secret set SNOWFLAKE_ACCOUNT
gh secret set SNOWFLAKE_USER_DEV
gh secret set SNOWFLAKE_PASSWORD_DEV
gh secret set SNOWFLAKE_USER_PROD
gh secret set SNOWFLAKE_PASSWORD_PROD
gh secret set SNOWFLAKE_CI_USER
gh secret set SNOWFLAKE_CI_PASSWORD

# Optional
gh secret set SLACK_WEBHOOK



## 8. References & Standards

* [Data Vault 2.0](https://danlinstedt.com/datavault-2-0/)
* [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/)
* [dbt Documentation](https://docs.getdbt.com/)
* [Apache Airflow](https://airflow.apache.org/)

```

---

# Architecture Diagram Plan

We can create a **visual diagram** using icons (AWS, Snowflake, Spark, Airflow, dbt, Docker, K8s):

**Flow (Left â†’ Right):**

1. **Data Sources (Banking Systems, CRM, Payments)**
   - Icons: Database, API, Kafka
2. **Ingestion Layer**
   - Spark Batch/Streaming â†’ S3 (Bronze)
   - Icon: Spark, S3
3. **Processing Layer**
   - Spark jobs â†’ Silver Layer
   - Icon: Spark, S3
4. **Data Vault Layer**
   - dbt builds Hub, Link, Satellite â†’ Snowflake
   - Icon: dbt, Snowflake
5. **Orchestration**
   - Airflow DAGs control ingestion, processing, dbt runs
   - Icon: Airflow
6. **Deployment & Infrastructure**
   - Terraform provision AWS infra
   - K8s runs Spark, Airflow, dbt containers
   - Icon: Terraform, K8s, Docker
7. **Analytics / BI**
   - Snowflake serves BI dashboards
   - Icon: BI tool / SQL query

---
https://www.youtube.com/watch?v=5NCywQcJ2r8


