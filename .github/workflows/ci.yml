name: CI Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  DBT_VERSION: '1.7.4'

jobs:
  # ============================================
  # Job 1: Code Quality & Linting
  # ============================================
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install black flake8 isort pylint
          
      - name: Run Black (Python formatter)
        run: black --check --diff dags/ spark_jobs/
        continue-on-error: true
        
      - name: Run Flake8 (Python linter)
        run: flake8 dags/ spark_jobs/ --max-line-length=120 --exclude=venv,__pycache__
        continue-on-error: true
        
      - name: Run isort (Import sorting)
        run: isort --check-only --diff dags/ spark_jobs/
        continue-on-error: true

  # ============================================
  # Job 2: Python Tests
  # ============================================
  python-tests:
    name: Python Unit Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-mock
          
      - name: Run pytest
        run: |
          pytest spark_jobs/tests/ \
            --cov=spark_jobs \
            --cov-report=xml \
            --cov-report=html \
            --verbose
            
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

  # ============================================
  # Job 3: Airflow DAG Validation
  # ============================================
  airflow-validation:
    name: Validate Airflow DAGs
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install Airflow
        run: |
          pip install apache-airflow==2.8.1
          pip install apache-airflow-providers-amazon[s3]==8.15.0
          pip install apache-airflow-providers-snowflake==5.3.0
          pip install apache-airflow-providers-apache-spark==4.6.0
          pip install astronomer-cosmos[dbt-snowflake]==1.3.0
          
      - name: Initialize Airflow DB
        run: |
          export AIRFLOW_HOME=${PWD}/airflow_home
          airflow db init
          
      - name: Validate DAG syntax
        run: |
          export AIRFLOW_HOME=${PWD}/airflow_home
          export AIRFLOW__CORE__LOAD_EXAMPLES=False
          export AIRFLOW__CORE__DAGS_FOLDER=${PWD}/dags
          
          # Test DAG imports
          python -c "
          import sys
          sys.path.insert(0, '${PWD}')
          from pathlib import Path
          import importlib.util
          
          dags_folder = Path('dags')
          errors = []
          
          for dag_file in dags_folder.glob('*.py'):
              if dag_file.name.startswith('_'):
                  continue
              try:
                  spec = importlib.util.spec_from_file_location(dag_file.stem, dag_file)
                  module = importlib.util.module_from_spec(spec)
                  spec.loader.exec_module(module)
                  print(f'✓ {dag_file.name} is valid')
              except Exception as e:
                  errors.append(f'✗ {dag_file.name}: {str(e)}')
                  print(f'✗ {dag_file.name}: {str(e)}')
          
          if errors:
              print(f'\n{len(errors)} DAG(s) failed validation')
              sys.exit(1)
          else:
              print(f'\nAll DAGs validated successfully')
          "
          
      - name: Run Airflow DAG tests
        run: |
          export AIRFLOW_HOME=${PWD}/airflow_home
          airflow dags list
          airflow dags list-import-errors

  # ============================================
  # Job 4: dbt Validation & Tests
  # ============================================
  dbt-validation:
    name: dbt Validation & Tests
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
          
      - name: Install dbt
        run: |
          pip install dbt-core==${{ env.DBT_VERSION }}
          pip install dbt-snowflake==1.7.1
          
      - name: Install dbt packages
        working-directory: ./dbt
        run: dbt deps
        
      - name: Create profiles.yml
        working-directory: ./dbt
        run: |
          mkdir -p ~/.dbt
          cat > ~/.dbt/profiles.yml << EOF
          banking_vault:
            outputs:
              ci:
                type: snowflake
                account: ${{ secrets.SNOWFLAKE_ACCOUNT }}
                user: ${{ secrets.SNOWFLAKE_CI_USER }}
                password: ${{ secrets.SNOWFLAKE_CI_PASSWORD }}
                role: TRANSFORMER
                database: ANALYTICS_DB_CI
                warehouse: COMPUTE_WH
                schema: VAULT_CI
                threads: 4
                client_session_keep_alive: False
            target: ci
          EOF
          
      - name: dbt Debug
        working-directory: ./dbt
        run: dbt debug --profiles-dir ~/.dbt
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_CI_USER: ${{ secrets.SNOWFLAKE_CI_USER }}
          SNOWFLAKE_CI_PASSWORD: ${{ secrets.SNOWFLAKE_CI_PASSWORD }}
          
      - name: dbt Parse (Check SQL Syntax)
        working-directory: ./dbt
        run: dbt parse --profiles-dir ~/.dbt
        
      - name: dbt Compile
        working-directory: ./dbt
        run: dbt compile --profiles-dir ~/.dbt
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_CI_USER: ${{ secrets.SNOWFLAKE_CI_USER }}
          SNOWFLAKE_CI_PASSWORD: ${{ secrets.SNOWFLAKE_CI_PASSWORD }}
          
      - name: Run dbt Tests (Dry Run)
        working-directory: ./dbt
        run: |
          # Run only staging models for CI
          dbt run --select staging --profiles-dir ~/.dbt --target ci
          dbt test --select staging --profiles-dir ~/.dbt --target ci
        env:
          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}
          SNOWFLAKE_CI_USER: ${{ secrets.SNOWFLAKE_CI_USER }}
          SNOWFLAKE_CI_PASSWORD: ${{ secrets.SNOWFLAKE_CI_PASSWORD }}
        continue-on-error: true
        
      - name: Generate dbt Docs
        working-directory: ./dbt
        run: dbt docs generate --profiles-dir ~/.dbt
        
      - name: Upload dbt Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dbt-artifacts
          path: |
            dbt/target/manifest.json
            dbt/target/catalog.json
            dbt/target/run_results.json

  # ============================================
  # Job 5: SQL Linting
  # ============================================
  sql-lint:
    name: SQL Linting (SQLFluff)
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Install SQLFluff
        run: pip install sqlfluff==3.0.0 sqlfluff-templater-dbt==3.0.0
        
      - name: Create SQLFluff config
        run: |
          cat > .sqlfluff << EOF
          [sqlfluff]
          templater = dbt
          dialect = snowflake
          exclude_rules = L003,L009
          max_line_length = 120
          
          [sqlfluff:templater:dbt]
          project_dir = dbt
          profiles_dir = dbt
          profile = banking_vault
          target = dev
          EOF
          
      - name: Lint dbt SQL files
        run: |
          sqlfluff lint dbt/models/ \
            --format github-annotation \
            --annotation-level warning
        continue-on-error: true

  # ============================================
  # Job 6: Security Scanning
  # ============================================
  security-scan:
    name: Security Scanning
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          
      - name: Upload Trivy results to GitHub Security
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: 'trivy-results.sarif'
          
      - name: Check for secrets in code
        uses: trufflesecurity/trufflehog@main
        with:
          path: ./
          base: ${{ github.event.repository.default_branch }}
          head: HEAD

  # ============================================
  # Job 7: Docker Build Test
  # ============================================
  docker-build:
    name: Docker Build Validation
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
        
      - name: Build Airflow Image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/airflow.Dockerfile
          push: false
          tags: banking-platform/airflow:test
          cache-from: type=gha
          cache-to: type=gha,mode=max
          
      - name: Build Spark Image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/spark.Dockerfile
          push: false
          tags: banking-platform/spark:test
          
      - name: Build dbt Image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/dbt.Dockerfile
          push: false
          tags: banking-platform/dbt:test

  # ============================================
  # Job 8: Final Status Check
  # ============================================
  ci-success:
    name: CI Pipeline Success
    runs-on: ubuntu-latest
    needs: 
      - code-quality
      - python-tests
      - airflow-validation
      - dbt-validation
      - sql-lint
      - security-scan
      - docker-build
    
    steps:
      - name: CI Passed
        run: |
          echo "✅ All CI checks passed successfully!"
          echo "Pipeline ready for merge."

